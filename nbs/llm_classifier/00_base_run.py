# AUTOGENERATED! DO NOT EDIT! File to edit: ../20250709_unsplit_ds.ipynb.

# %% auto 0
__all__ = ['iskaggle', 'isremote', 'WANDB_PROJECT_NAME', 'INPUT_DIR', 'OUTPUT_DIR', 'MODEL_DIR', 'cols', 'df_train', 'df_test',
           'model_path', 'tokenizer', 'model', 'texts', 'dataset', 'dataloader', 'token_counts', 'train_texts',
           'val_texts', 'train_labels', 'val_labels', 'train_dataset', 'val_dataset', 'test_dataset', 'train_params',
           'final_model_path', 'timestamp', 'run_name', 'training_args', 'trainer', 'text', 'inputs', 'outputs',
           'predictions', 'preds', 'all_probabilities', 'test_dataloader', 'final_probs', 'submission_df',
           'df_for_kaggle', 'setup_environment', 'null_clean', 'apply_bert_fmt', 'parse_args', 'TokenizeDataset',
           'LLMDataset', 'compute_metrics', 'TrainParams', 'get_train_params', 'load_model']

# %% ../20250709_unsplit_ds.ipynb 2
import os

iskaggle = os.path.exists("/kaggle/input")
isremote = os.path.exists("/home/ubuntu")

# %% ../20250709_unsplit_ds.ipynb 3
import wandb
import argparse
import os
import shutil
import fastkaggle
from pathlib import Path
import numpy as np
import pandas as pd
import polars as pl
from datasets import Dataset
import torch  # base
import torch.nn.functional as F
import json
from transformers.trainer import Trainer
from transformers.training_args import TrainingArguments
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
)
from pydantic import BaseModel
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset as TorchDataset
import evaluate
from torch.utils.data import DataLoader
from datetime import datetime
import subprocess
from sklearn.metrics import accuracy_score
from typing import List, Optional, Dict, Tuple, Union, Literal

# %% ../20250709_unsplit_ds.ipynb 6
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("Using MPS (Apple Silicon GPU)")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print("Using CUDA (NVIDIA GPU)")
else:
    device = torch.device("cpu")
    print("Using CPU")

print(f"Device: {device}")

# %% ../20250709_unsplit_ds.ipynb 7
if not iskaggle and not isremote:
    data_base_path = Path("./data")
    comp_name = "llm-classification-finetuning"
    datapath = data_base_path / comp_name
    if not os.path.exists(datapath) and not datapath.exists():
        install_path = fastkaggle.setup_comp(comp_name)
        shutil.move(install_path, datapath)

# %% ../20250709_unsplit_ds.ipynb 8
if isremote:
    if os.getcwd() == "/home/ubuntu":
        os.chdir("./llm-classification-finetuning")

# %% ../20250709_unsplit_ds.ipynb 10
WANDB_PROJECT_NAME = "kaggle-llm-classification"


def setup_environment():
    """Detect environment and set up paths for both local and Kaggle"""

    if iskaggle:
        print("Running on Kaggle")

        INPUT_DIR = "/kaggle/input/llm-classification-finetuning"
        OUTPUT_DIR = "/kaggle/working"
        MODEL_DIR = "/kaggle/working/models"

        os.environ["WANDB_MODE"] = "disabled"
    else:
        print("üíª Running locally")

        INPUT_DIR = "./data/llm-classification-finetuning"
        OUTPUT_DIR = "./output"
        MODEL_DIR = "./models"

        os.environ["WANDB_PROJECT"] = WANDB_PROJECT_NAME
        os.environ["WANDB_LOG_MODEL"] = "false"
        os.environ["WANDB_WATCH"] = "false"
        # os.environ["WANDB_MODE"] = "disabled"

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(MODEL_DIR, exist_ok=True)
    # to kill warning when running in notebooks
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    return INPUT_DIR, OUTPUT_DIR, MODEL_DIR


INPUT_DIR, OUTPUT_DIR, MODEL_DIR = setup_environment()
print(f"Input directory: {INPUT_DIR}")
print(f"Output directory: {OUTPUT_DIR}")
print(f"Model directory: {MODEL_DIR}")

# %% ../20250709_unsplit_ds.ipynb 12
try:
    df_train = pl.read_csv(f"{INPUT_DIR}/train.csv")
    df_test = pl.read_csv(f"{INPUT_DIR}/test.csv")
    df_sample = pl.read_csv(f"{INPUT_DIR}/sample_submission.csv")

    print("Data loaded successfully!")
    print(f"Train: {df_train.shape}")
    print(f"Test: {df_test.shape}")
    print(f"Sample submission: {df_sample.shape}")

except FileNotFoundError as e:
    print(f" Data file not found: {e}")
    print(f" Make sure data is in: {INPUT_DIR}")

    if os.path.exists(INPUT_DIR):
        files = os.listdir(INPUT_DIR)
        print(f"üìÅ Files in {INPUT_DIR}: {files}")

# %% ../20250709_unsplit_ds.ipynb 17
cols = ["prompt", "response_a", "response_b"]
if isinstance(df_train["prompt"][0], str):
    first_prompt_cell = df_train["prompt"][0]
    if "[" in first_prompt_cell and "]" in first_prompt_cell:
        df_train = df_train.with_columns([pl.col(col).str.json_decode() for col in cols])
        # explode
        df_train = df_train.explode(cols)
        # join
        # df_train = df_train.with_columns([pl.col(col).list.join(" ") for col in cols])

# %% ../20250709_unsplit_ds.ipynb 20
def null_clean(df: pl.DataFrame):
    df = df.drop_nulls(subset=["prompt", "response_a", "response_b"])
    return df

# %% ../20250709_unsplit_ds.ipynb 21
df_train = null_clean(df_train)
df_test = null_clean(df_test)

# %% ../20250709_unsplit_ds.ipynb 25
def apply_bert_fmt(df: pl.DataFrame):
    df = df.with_columns(
        (
           "[CLS]" + pl.col("prompt") + "[SEP]" + pl.col("response_a") + "[SEP]" + pl.col("response_b")
        ).alias("text")
    )
    return df

# %% ../20250709_unsplit_ds.ipynb 27
df_train = df_train.with_columns(
    pl.when(pl.col("winner_model_a") == 1)
    .then(pl.lit(0))
    .when(pl.col("winner_model_b") == 1)
    .then(pl.lit(1))
    .when(pl.col("winner_tie") == 1)
    .then(pl.lit(2))
    .otherwise(pl.lit(None))
    .alias("label")
)

# %% ../20250709_unsplit_ds.ipynb 28
df_train = apply_bert_fmt(df_train)
df_test = apply_bert_fmt(df_test)

# %% ../20250709_unsplit_ds.ipynb 32
def parse_args():
  parser = argparse.ArgumentParser(description="LLM Classification BERT Finetuning")
  parser.add_argument("--lr", type=float, default=2e-5, help="Learning rate for training")
  parser.add_argument("--bs", type=int, default=8, help="Batch size for training")
  parser.add_argument("--epoch", type=int, default=2, help="Number of training epochs")
  parser.add_argument("--max_steps", type=int, default=None, help="Maximum training steps")
  parser.add_argument("--model_path", type=str, default="answerdotai/ModernBERT-base", help="Model path or name")
  parser.add_argument("--output_dir", type=str, default=None, help="Output directory (overrides default)")
  parser.add_argument("--disable_wandb", action="store_true", help="Disable wandb logging")
  parser.add_argument("--max_len", type=int,default=8192, help="Max sequence length for model")
  parser.add_argument("--bf16", type=bool, default=False, help="Use bf16 precision")
  parser.add_argument("--fp16", type=bool, default=False, help="Use fp16 precision")

  args, unknown = parser.parse_known_args()
  return args

if __name__ == "__main__":
  args = parse_args()
else:
  class Args:
    lr = 2e-5
    bs = 8
    epochs = 2
    model_path = "answerdotai/ModernBERT-base"
    max_len = 8192
    bf16: bool
    fp16: bool

  args = Args()

# %% ../20250709_unsplit_ds.ipynb 33
model_path = args.model_path
print("Loading BERT from Hugging Face...")

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3)
model.to(device)
model.device

# %% ../20250709_unsplit_ds.ipynb 34
class TokenizeDataset(TorchDataset):
    def __init__(self, texts, tokenizer):
      self.texts = texts
      self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
       tokens = self.tokenizer.tokenize(self.texts[idx])
       return len(tokens)

# %% ../20250709_unsplit_ds.ipynb 35
texts = df_train['text'].to_list()
dataset = TokenizeDataset(texts, tokenizer)
dataloader = DataLoader(dataset, batch_size=1000, num_workers=4)
token_counts = []
for batch in dataloader:
    token_counts.extend(batch.tolist())

# %% ../20250709_unsplit_ds.ipynb 36
df_train = df_train.with_columns(pl.Series("token_count", token_counts))

# %% ../20250709_unsplit_ds.ipynb 37
# for now just filter anything over the context window
df_train.filter(pl.col("token_count") < 510)

# %% ../20250709_unsplit_ds.ipynb 38
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df_train["text"], df_train["label"], test_size=0.1, random_state=42
)

# %% ../20250709_unsplit_ds.ipynb 44
class LLMDataset(torch.utils.data.Dataset):
    def __init__(self, texts, tokenizer, labels=None, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    # encoding on the fly here due to issues with memory on kaggle
    # when pre-tokenizing
    def __getitem__(self, idx):
        text = self.texts[idx]
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt",
        )

        item = {key: val.squeeze() for key, val in encoding.items()}

        if self.labels is not None:
            item["labels"] = torch.tensor(self.labels[idx])

        return item

    def __len__(self):
        return len(self.texts)

# %% ../20250709_unsplit_ds.ipynb 45
train_dataset = LLMDataset(list(train_texts), tokenizer, list(train_labels), max_length=args.max_len)
val_dataset = LLMDataset(list(val_texts), tokenizer, list(val_labels), max_length=args.max_len)
test_dataset = LLMDataset(list(df_test["text"]), tokenizer, max_length=args.max_len )

# %% ../20250709_unsplit_ds.ipynb 46
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {"accuracy": accuracy_score(labels, preds)}

# %% ../20250709_unsplit_ds.ipynb 48
class TrainParams(BaseModel):
  learning_rate: float = 2e-5
  per_device_train_batch_size: int = 8
  per_device_eval_batch_size: int = 8
  num_train_epochs: Optional[int] = 2
  weight_decay: float = 0.01
  max_steps: Optional[int] = None
  eval_strategy: Literal["epoch", "steps", "no"] = "epoch"
  save_strategy: Literal["no", "steps", "epoch"] = "epoch"
  load_best_model_at_end: bool = True
  metric_for_best_model: Literal["accuracy"] = "accuracy"
  logging_steps: int = 10
  logging_first_step: bool = False
  dataloader_num_workers: int = 0 
  bf16: bool = False
  fp16: bool = False

# %% ../20250709_unsplit_ds.ipynb 49
def get_train_params() -> TrainParams:
    """Get training parameters based on environment"""
    # remote gpu
    if isremote:
        return TrainParams(
            learning_rate=args.lr,
            per_device_train_batch_size=args.bs,
            per_device_eval_batch_size=args.bs,
            logging_steps=20,
            bf16=args.bf16
        )
    # kaggle 
    if iskaggle:  
        return TrainParams(
            per_device_train_batch_size=args.bs,
            per_device_eval_batch_size=args.bs,
            load_best_model_at_end=True,
            save_strategy="no",
        )
    # local mps
    else:      
        return TrainParams(
            per_device_train_batch_size=args.bs,
            per_device_eval_batch_size=args.bs,
            max_steps=100,
            num_train_epochs=None,  
            eval_strategy="no",
            logging_steps=2,
            save_strategy="no",
                )

# %% ../20250709_unsplit_ds.ipynb 51
train_params = get_train_params()

# %% ../20250709_unsplit_ds.ipynb 52
final_model_path = f"{MODEL_DIR}/final"

timestamp = datetime.now().strftime("%Y%m%d-%H%M")
run_name = f"bert-classification-{timestamp}"

# %% ../20250709_unsplit_ds.ipynb 53
if os.environ.get("WANDB_MODE") != "disabled":
    import wandb
    
    tags = [
        f"model_{args.model_path.split('/')[-1]}",
        f"lr_{train_params.learning_rate}",
        f"bs_{train_params.per_device_train_batch_size}",
        f"epochs_{train_params.num_train_epochs}" if train_params.num_train_epochs else f"steps_{train_params.max_steps}",
    ]

    wandb.init(
        project=WANDB_PROJECT_NAME,
        name=run_name    ,
        tags=tags
    )
    print(f"üè∑Ô∏è  Tagged run with: {tags}")

# %% ../20250709_unsplit_ds.ipynb 54
training_args = TrainingArguments(
    output_dir=f"{OUTPUT_DIR}/results",
    run_name=run_name,
    report_to="wandb" if os.environ.get("WANDB_MODE") != "disabled" else [],
    **train_params.model_dump(exclude_none=True)
)

# %% ../20250709_unsplit_ds.ipynb 55
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    processing_class=tokenizer,
    compute_metrics=compute_metrics,
)

# %% ../20250709_unsplit_ds.ipynb 56
trainer.train()

# %% ../20250709_unsplit_ds.ipynb 57
trainer.save_model(final_model_path)
tokenizer.save_pretrained(final_model_path)
# if os.environ.get("WANDB_MODE") != "disabled":
#     wandb.log({"final_eval": eval_results})
#     wandb.save(f"{final_model_path}/*")
# wandb.finish()

# %% ../20250709_unsplit_ds.ipynb 59
def load_model(model_path):
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    return model, tokenizer


model, tokenizer = load_model(final_model_path)
model.to(device)

text = "This is a test sentence"
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
outputs = model(**inputs.to(device))
predictions = outputs.logits

# %% ../20250709_unsplit_ds.ipynb 61
preds = F.softmax(predictions, dim=-1)

# %% ../20250709_unsplit_ds.ipynb 63
all_probabilities = []
test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)

with torch.no_grad():
    for batch in test_dataloader:
        inputs = {k: v.to(model.device) for k, v in batch.items() if k != "labels"}

        outputs = model(**inputs)
        probabilities = F.softmax(outputs.logits, dim=-1)
        all_probabilities.extend(probabilities.cpu().numpy())
final_probs = np.vstack(all_probabilities)

# %% ../20250709_unsplit_ds.ipynb 66
submission_df = df_test
submission_df = submission_df.with_columns(
    pl.lit(final_probs[:, 0]).alias("winner_model_a"),
    pl.lit(final_probs[:, 1]).alias("winner_model_b"),
    pl.lit(final_probs[:, 2]).alias("winner_tie"),
)
submission_df = submission_df[["id", "winner_model_a", "winner_model_b", "winner_tie"]]
submission_df

# %% ../20250709_unsplit_ds.ipynb 67
df_for_kaggle = submission_df.to_pandas()

# %% ../20250709_unsplit_ds.ipynb 68
df_for_kaggle.to_csv("submission.csv", index=False)
df_for_kaggle.head()
