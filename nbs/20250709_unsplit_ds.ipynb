{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle dependencies\n",
    "########################\n",
    "# pip install fastkaggle\n",
    "# pip install wandb\n",
    "# pip install polars\n",
    "# pip install datasets\n",
    "# pip install scikit-learn\n",
    "# pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "iskaggle = os.path.exists(\"/kaggle/input\")\n",
    "isremote = os.path.exists(\"/home/ubuntu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "import shutil\n",
    "import fastkaggle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datasets import Dataset\n",
    "import torch  # base\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Response Scoring with BERT\n",
    "\n",
    "This notebook is for the [llm-classification-finetuning](https://www.kaggle.com/competitions/llm-classification-finetuning) competition on kaggle. It's a quick fine-tune of the `bert-base-uncased` model to predict which LLM response is preferrable. There are probably better models and approaches for this, but BERT does pretty well on its own without a whole lot of intervetion.\n",
    "\n",
    "For me, this was more of a quick experiment in getting some external dependincies set up in a kaggle `code competition` notebook.\n",
    "\n",
    "- To get additional libraries installed, open the notebook in kaggle and select `Install Dependencies` from the `Add-On` menu.\n",
    "\n",
    "- To run the BERT model offline, add this dataset to your notebook dependencies:\n",
    "  - https://www.kaggle.com/datasets/xhlulu/huggingface-bert\n",
    "\n",
    "You can see later on how to reference the local model location in the kaggle notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle the GPU handoff for all the different machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not iskaggle and not isremote:\n",
    "    data_base_path = Path(\"./data\")\n",
    "    comp_name = \"llm-classification-finetuning\"\n",
    "    datapath = data_base_path / comp_name\n",
    "    if not os.path.exists(datapath) and not datapath.exists():\n",
    "        install_path = fastkaggle.setup_comp(comp_name)\n",
    "        shutil.move(install_path, datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isremote:\n",
    "    if os.getcwd() == \"/home/ubuntu\":\n",
    "        os.chdir(\"./llm-classification-finetuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Kaggle/Local Env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_PROJECT_NAME = \"kaggle-llm-classification\"\n",
    "\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Detect environment and set up paths for both local and Kaggle\"\"\"\n",
    "\n",
    "    if iskaggle:\n",
    "        print(\"Running on Kaggle\")\n",
    "\n",
    "        INPUT_DIR = \"/kaggle/input/llm-classification-finetuning\"\n",
    "        OUTPUT_DIR = \"/kaggle/working\"\n",
    "        MODEL_DIR = \"/kaggle/working/models\"\n",
    "\n",
    "        os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "    else:\n",
    "        print(\"üíª Running locally\")\n",
    "\n",
    "        INPUT_DIR = \"./data/llm-classification-finetuning\"\n",
    "        OUTPUT_DIR = \"./output\"\n",
    "        MODEL_DIR = \"./models\"\n",
    "\n",
    "        os.environ[\"WANDB_PROJECT\"] = WANDB_PROJECT_NAME\n",
    "        os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
    "        os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "        # os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    # to kill warning when running in notebooks\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    return INPUT_DIR, OUTPUT_DIR, MODEL_DIR\n",
    "\n",
    "\n",
    "INPUT_DIR, OUTPUT_DIR, MODEL_DIR = setup_environment()\n",
    "print(f\"Input directory: {INPUT_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_train = pl.read_csv(f\"{INPUT_DIR}/train.csv\")\n",
    "    df_test = pl.read_csv(f\"{INPUT_DIR}/test.csv\")\n",
    "    df_sample = pl.read_csv(f\"{INPUT_DIR}/sample_submission.csv\")\n",
    "\n",
    "    print(\"Data loaded successfully!\")\n",
    "    print(f\"Train: {df_train.shape}\")\n",
    "    print(f\"Test: {df_test.shape}\")\n",
    "    print(f\"Sample submission: {df_sample.shape}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\" Data file not found: {e}\")\n",
    "    print(f\" Make sure data is in: {INPUT_DIR}\")\n",
    "\n",
    "    if os.path.exists(INPUT_DIR):\n",
    "        files = os.listdir(INPUT_DIR)\n",
    "        print(f\"üìÅ Files in {INPUT_DIR}: {files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_width_chars(300)\n",
    "pl.Config.set_fmt_str_lengths(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"prompt\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try concatenating full conversations and full answer sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "if isinstance(df_train[\"prompt\"][0], str):\n",
    "    first_prompt_cell = df_train[\"prompt\"][0]\n",
    "    if \"[\" in first_prompt_cell and \"]\" in first_prompt_cell:\n",
    "        df_train = df_train.with_columns([pl.col(col).str.json_decode() for col in cols])\n",
    "        # explode\n",
    "        df_train = df_train.explode(cols)\n",
    "        # join\n",
    "        # df_train = df_train.with_columns([pl.col(col).list.join(\" \") for col in cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"prompt\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any empty conversational turns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_clean(df: pl.DataFrame):\n",
    "    df = df.drop_nulls(subset=[\"prompt\", \"response_a\", \"response_b\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = null_clean(df_train)\n",
    "df_test = null_clean(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format for `BERT` by concatenating the prompt and both responses with `[SEP]` tokens in between\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "([\"CLS\"] + df_train[\"prompt\"] + \"[SEP]\" + df_train[\"response_a\"] + \"[SEP]\" + df_train[\"response_b\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bert_fmt(df: pl.DataFrame):\n",
    "    df = df.with_columns(\n",
    "        (\n",
    "           \"[CLS]\" + pl.col(\"prompt\") + \"[SEP]\" + pl.col(\"response_a\") + \"[SEP]\" + pl.col(\"response_b\")\n",
    "        ).alias(\"text\")\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condense to a single target column and remap to 0,1,2 as possible dep vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.with_columns(\n",
    "    pl.when(pl.col(\"winner_model_a\") == 1)\n",
    "    .then(pl.lit(0))\n",
    "    .when(pl.col(\"winner_model_b\") == 1)\n",
    "    .then(pl.lit(1))\n",
    "    .when(pl.col(\"winner_tie\") == 1)\n",
    "    .then(pl.lit(2))\n",
    "    .otherwise(pl.lit(None))\n",
    "    .alias(\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = apply_bert_fmt(df_train)\n",
    "df_test = apply_bert_fmt(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the training data into training and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_train[\"text\"], df_train[\"label\"], test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts.shape, val_texts.shape, train_labels.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a code competition, which means that internet access is cut off when running the notebook. Some people have kindly created kaggle `datasets` of some `BERT` models. I've added the relevant dataset as a dependecy, so in theory this will use a \"local\" version of the model in kaggle.\n",
    "\n",
    "This doesn't always work perfectly. Sometimes it needs the nudge of saving it as a new draft on kaggle to give the `dataset` time to load.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if iskaggle:\n",
    "    model_path = \"../input/huggingface-bert/bert-base-uncased\"\n",
    "    print(\"Loading BERT from Kaggle model input...\")\n",
    "\n",
    "else:\n",
    "    model_path = \"bert-base-uncased\"\n",
    "    print(\"Loading BERT from Hugging Face...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "model.to(device)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_enc = tokenizer(train_texts[0], truncation=True, padding=True)\n",
    "ex_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer, labels=None, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    # encoding on the fly here due to issues with memory on kaggle\n",
    "    # when pre-tokenizing\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LLMDataset(list(train_texts), tokenizer, list(train_labels))\n",
    "val_dataset = LLMDataset(list(val_texts), tokenizer, list(val_labels))\n",
    "test_dataset = LLMDataset(list(df_test[\"text\"]), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = f\"{MODEL_DIR}/final\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "run_name = f\"bert-classification-{timestamp}\"\n",
    "test_run = False\n",
    "if not iskaggle and test_run:\n",
    "    # Quick run to test pipeline\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/results\",\n",
    "        run_name=run_name,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        max_steps=2000,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        logging_steps=2,\n",
    "        logging_first_step=True,\n",
    "        report_to=\"wandb\" if os.environ.get(\"WANDB_MODE\") != \"disabled\" else [],\n",
    "        dataloader_num_workers=0,  # Important for Kaggle compatibility\n",
    "    )\n",
    "\n",
    "else:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/results\",\n",
    "        run_name=run_name,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        logging_steps=5,\n",
    "        logging_first_step=True,\n",
    "        report_to=\"wandb\" if os.environ.get(\"WANDB_MODE\") != \"disabled\" else [],\n",
    "        dataloader_num_workers=0,  # Important for Kaggle compatibility\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# trainer.save_model(final_model_path)\n",
    "# tokenizer.save_pretrained(final_model_path)\n",
    "# if os.environ.get(\"WANDB_MODE\") != \"disabled\":\n",
    "#     wandb.log({\"final_eval\": eval_results})\n",
    "#     wandb.save(f\"{final_model_path}/*\")\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "model, tokenizer = load_model(final_model_path)\n",
    "model.to(device)\n",
    "\n",
    "text = \"This is a test sentence\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = model(**inputs.to(device))\n",
    "predictions = outputs.logits\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = F.softmax(predictions, dim=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_probabilities = []\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "final_probs = np.vstack(all_probabilities)\n",
    "final_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_probs[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = df_test\n",
    "submission_df = submission_df.with_columns(\n",
    "    pl.lit(final_probs[:, 0]).alias(\"winner_model_a\"),\n",
    "    pl.lit(final_probs[:, 1]).alias(\"winner_model_b\"),\n",
    "    pl.lit(final_probs[:, 2]).alias(\"winner_tie\"),\n",
    ")\n",
    "submission_df = submission_df[[\"id\", \"winner_model_a\", \"winner_model_b\", \"winner_tie\"]]\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_kaggle = submission_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_kaggle.to_csv(\"submission.csv\", index=False)\n",
    "df_for_kaggle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Notebook to Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_notebook_cli():\n",
    "    username = \"peterbull\"\n",
    "    comp = \"llm-classification-finetuning\"\n",
    "    notebook_file = \"20250709_unsplit_ds.ipynb\"\n",
    "    metadata = {\n",
    "        \"id\": f\"{username}/llm-classification-bert-finetuning\",\n",
    "        \"title\": \"LLM Classification BERT Finetuning\",\n",
    "        \"code_file\": notebook_file,\n",
    "        \"language\": \"python\",\n",
    "        \"kernel_type\": \"notebook\",\n",
    "        \"is_private\": True,\n",
    "        \"enable_gpu\": True,\n",
    "        \"enable_internet\": False,  # required for kaggle code competition\n",
    "        \"dataset_sources\": [],\n",
    "        \"competition_sources\": [f\"competitions/{comp}\"],\n",
    "        \"kernel_sources\": [],\n",
    "    }\n",
    "\n",
    "    with open(\"kernel-metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    if not os.path.exists(notebook_file):\n",
    "        print(\" Notebook file not found!\")\n",
    "        print(\" Files in current directory:\")\n",
    "        for f in os.listdir(\".\"):\n",
    "            if f.endswith(\".ipynb\"):\n",
    "                print(f\"{f}\")\n",
    "        return\n",
    "\n",
    "    print(\"Pushing to Kaggle...\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"kaggle\", \"kernels\", \"push\", \"-p\", \".\"], capture_output=True, text=True, timeout=300\n",
    "        )\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Notebook pushed successfully!\")\n",
    "            print(result.stdout)\n",
    "            print(\n",
    "                f\"üîó View at: https://www.kaggle.com/code/{username}/llm-classification-bert-finetuning\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"Error pushing notebook:\")\n",
    "            print(result.stderr)\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"Upload timed out after 5 minutes\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Kaggle CLI not found. Install with: pip install kaggle\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "if not iskaggle:\n",
    "    push_notebook_cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
