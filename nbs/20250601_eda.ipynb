{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llm Classification Finetuning EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastkaggle\n",
    "import os\n",
    "import shutil\n",
    "import polars as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = fastkaggle.import_kaggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm-classification-finetuning.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "data_base_path = Path(\"./data\")\n",
    "comp_name = \"llm-classification-finetuning\"\n",
    "install_path = fastkaggle.setup_comp(comp_name)\n",
    "datapath = data_base_path / comp_name\n",
    "if os.path.exists(install_path):\n",
    "    shutil.move(install_path, data_base_path / comp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Description\n",
    "\n",
    "The competition dataset consists of user interactions from the ChatBot Arena. In each user interaction a judge provides one or more prompts to two different large language models, and then indicates which of the models gave the more satisfactory response. The goal of the competition is to predict the preferences of the judges and determine the likelihood that a given prompt/response pair is selected as the winner.\n",
    "\n",
    "Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. There are 55K rows in the training data, and you can expect roughly 25,000 rows in the test set.\n",
    "\n",
    "## Files\n",
    "\n",
    "### train.csv\n",
    "\n",
    "- **id** - A unique identifier for the row.\n",
    "- **model\\_[a/b]** - The identity of model\\_[a/b]. Included in train.csv but not test.csv.\n",
    "- **prompt** - The prompt that was given as an input (to both models).\n",
    "- **response\\_[a/b]** - The response from model\\_[a/b] to the given prompt.\n",
    "- **winner*model*[a/b/tie]** - Binary columns marking the judge's selection. The ground truth target column.\n",
    "\n",
    "### test.csv\n",
    "\n",
    "- **id**\n",
    "- **prompt**\n",
    "- **response\\_[a/b]**\n",
    "\n",
    "### sample_submission.csv\n",
    "\n",
    "A submission file in the correct format.\n",
    "\n",
    "- **id**\n",
    "- **winner*model*[a/b/tie]** - This is what is predicted from the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>winner_model_a</th><th>winner_model_b</th><th>winner_tie</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>136060</td><td>0.333333</td><td>0.333333</td><td>0.333333</td></tr><tr><td>211333</td><td>0.333333</td><td>0.333333</td><td>0.333333</td></tr><tr><td>1233961</td><td>0.333333</td><td>0.333333</td><td>0.333333</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 4)\n",
       "┌─────────┬────────────────┬────────────────┬────────────┐\n",
       "│ id      ┆ winner_model_a ┆ winner_model_b ┆ winner_tie │\n",
       "│ ---     ┆ ---            ┆ ---            ┆ ---        │\n",
       "│ i64     ┆ f64            ┆ f64            ┆ f64        │\n",
       "╞═════════╪════════════════╪════════════════╪════════════╡\n",
       "│ 136060  ┆ 0.333333       ┆ 0.333333       ┆ 0.333333   │\n",
       "│ 211333  ┆ 0.333333       ┆ 0.333333       ┆ 0.333333   │\n",
       "│ 1233961 ┆ 0.333333       ┆ 0.333333       ┆ 0.333333   │\n",
       "└─────────┴────────────────┴────────────────┴────────────┘"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.read_csv(datapath / \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
