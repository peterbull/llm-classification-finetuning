{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "iskaggle = os.path.exists(\"/kaggle/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "import shutil\n",
    "import fastkaggle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datasets import Dataset\n",
    "import torch  # base\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Response Scoring with BERT\n",
    "\n",
    "This notebook is for the [llm-classification-finetuning](https://www.kaggle.com/competitions/llm-classification-finetuning) competition on kaggle. It's a quick fine-tune of the `bert-base-uncased` model to predict which LLM response is preferrable. There are probably better models and approaches for this, but BERT does pretty well on its own without a whole lot of intervetion.\n",
    "\n",
    "For me, this was more of a quick experiment in getting some external dependincies set up in a kaggle `code competition` notebook.\n",
    "\n",
    "- To get additional libraries installed, open the notebook in kaggle and select `Install Dependencies` from the `Add-On` menu.\n",
    "\n",
    "- To run the BERT model offline, add this dataset to your notebook dependencies:\n",
    "  - https://www.kaggle.com/datasets/xhlulu/huggingface-bert\n",
    "\n",
    "You can see later on how to reference the local model location in the kaggle notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle the GPU handoff for all the different machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not iskaggle:\n",
    "    data_base_path = Path(\"./data\")\n",
    "    comp_name = \"llm-classification-finetuning\"\n",
    "    datapath = data_base_path / comp_name\n",
    "    if not os.path.exists(datapath) and not datapath.exists():\n",
    "        install_path = fastkaggle.setup_comp(comp_name)\n",
    "        shutil.move(install_path, datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Kaggle/Local Env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’» Running locally\n",
      "Input directory: ./data/llm-classification-finetuning\n",
      "Output directory: ./output\n",
      "Model directory: ./models\n"
     ]
    }
   ],
   "source": [
    "WANDB_PROJECT_NAME = \"kaggle-llm-classification\"\n",
    "\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Detect environment and set up paths for both local and Kaggle\"\"\"\n",
    "\n",
    "    if iskaggle:\n",
    "        print(\"Running on Kaggle\")\n",
    "\n",
    "        INPUT_DIR = \"/kaggle/input/llm-classification-finetuning\"\n",
    "        OUTPUT_DIR = \"/kaggle/working\"\n",
    "        MODEL_DIR = \"/kaggle/working/models\"\n",
    "\n",
    "        os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "    else:\n",
    "        print(\"ğŸ’» Running locally\")\n",
    "\n",
    "        INPUT_DIR = \"./data/llm-classification-finetuning\"\n",
    "        OUTPUT_DIR = \"./output\"\n",
    "        MODEL_DIR = \"./models\"\n",
    "\n",
    "        os.environ[\"WANDB_PROJECT\"] = WANDB_PROJECT_NAME\n",
    "        os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
    "        os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    # to kill warning when running in notebooks\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    return INPUT_DIR, OUTPUT_DIR, MODEL_DIR\n",
    "\n",
    "\n",
    "INPUT_DIR, OUTPUT_DIR, MODEL_DIR = setup_environment()\n",
    "print(f\"Input directory: {INPUT_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Train: (57477, 9)\n",
      "Test: (3, 4)\n",
      "Sample submission: (3, 4)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_train = pl.read_csv(f\"{INPUT_DIR}/train.csv\")\n",
    "    df_test = pl.read_csv(f\"{INPUT_DIR}/test.csv\")\n",
    "    df_sample = pl.read_csv(f\"{INPUT_DIR}/sample_submission.csv\")\n",
    "\n",
    "    print(\"Data loaded successfully!\")\n",
    "    print(f\"Train: {df_train.shape}\")\n",
    "    print(f\"Test: {df_test.shape}\")\n",
    "    print(f\"Sample submission: {df_sample.shape}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\" Data file not found: {e}\")\n",
    "    print(f\" Make sure data is in: {INPUT_DIR}\")\n",
    "\n",
    "    if os.path.exists(INPUT_DIR):\n",
    "        files = os.listdir(INPUT_DIR)\n",
    "        print(f\"ğŸ“ Files in {INPUT_DIR}: {files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.Config.set_tbl_width_chars(300)\n",
    "pl.Config.set_fmt_str_lengths(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>model_a</th><th>model_b</th><th>prompt</th><th>response_a</th><th>response_b</th><th>winner_model_a</th><th>winner_model_b</th><th>winner_tie</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>30192</td><td>&quot;gpt-4-1106-preview&quot;</td><td>&quot;gpt-4-0613&quot;</td><td>&quot;[&quot;Is it morally right to try to have a certain percentage of females on managerial positions?&quot;,&quot;OK, does pineapple belong on a pizza? Relax and give me fun answer.&quot;]&quot;</td><td>&quot;[&quot;The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\\n\\nHere are some arguments in favor of and against such policies:\\n\\n**Argumeâ€¦</td><td>&quot;[&quot;As an AI, I don&#x27;t have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better deciâ€¦</td><td>1</td><td>0</td><td>0</td></tr><tr><td>53567</td><td>&quot;koala-13b&quot;</td><td>&quot;gpt-4-0613&quot;</td><td>&quot;[&quot;What is the difference between marriage license and marriage certificate?&quot;,&quot;How can I get both of them as quick as possible in California &quot;,&quot;What is the minimal time to get them? 1 day or 10 days?&quot;]&quot;</td><td>&quot;[&quot;A marriage license is a legal document that allows a couple to get married. It is issued by a government agency, such as a county clerk&#x27;s office or a state government, and is valid for a certain period of time, usually one year. After the marriage has taken place, the couple must obtain a marriageâ€¦</td><td>&quot;[&quot;A marriage license and a marriage certificate are two different legal documents that have separate purposes.\\n\\n1. Marriage License: A marriage license is a legal document that gives a couple permission to get married. It&#x27;s usually obtained from local government or court officials before the weddiâ€¦</td><td>0</td><td>1</td><td>0</td></tr><tr><td>65089</td><td>&quot;gpt-3.5-turbo-0613&quot;</td><td>&quot;mistral-medium&quot;</td><td>&quot;[&quot;explain function calling. how would you call a function?&quot;]&quot;</td><td>&quot;[&quot;Function calling is the process of invoking or executing a function in a programming language. To call a function, you need to use the function&#x27;s name followed by parentheses \\&quot;()\\&quot;. Within the parentheses, you can pass arguments if the function requires any.\\n\\nHere is an example of how to call aâ€¦</td><td>&quot;[&quot;Function calling is the process of invoking a function in a program. A function is a block of code that performs a specific task and can be reused multiple times in a program. Functions help to organize and modularize code, making it more readable and maintainable.\\n\\nTo call a function, you simplâ€¦</td><td>0</td><td>0</td><td>1</td></tr><tr><td>96401</td><td>&quot;llama-2-13b-chat&quot;</td><td>&quot;mistral-7b-instruct&quot;</td><td>&quot;[&quot;How can I create a test set for a very rare category? I want to build a classifier.&quot;]&quot;</td><td>&quot;[&quot;Creating a test set for a very rare category can be challenging, as it may not be possible to collect a sufficient number of examples for the rare category. However, there are several strategies you can use to create a test set for a rare category:\\n\\n1. Use synthetic data: If the rare category isâ€¦</td><td>&quot;[&quot;When building a classifier for a very rare category, creating a test set can be challenging due to the limited amount of data available. Here are some steps you can take to create a test set for a rare category:\\n\\n1. Gather as much data as possible: The more data you have, the easier it will be tâ€¦</td><td>1</td><td>0</td><td>0</td></tr><tr><td>198779</td><td>&quot;koala-13b&quot;</td><td>&quot;gpt-3.5-turbo-0314&quot;</td><td>&quot;[&quot;What is the best way to travel from Tel-Aviv to Jerusalem? Car? Bus? Plane?&quot;]&quot;</td><td>&quot;[&quot;The best way to travel from Tel Aviv to Jerusalem depends on your personal preference and the availability of transportation options. All the options you have mentioned are valid options, but here are some details to help you make your decision:\\n\\n*&nbsp;&nbsp;&nbsp;By car: Traveling by car is the quickest way â€¦</td><td>&quot;[&quot;The best way to travel from Tel-Aviv to Jerusalem depends on your personal preferences and circumstances. \\n\\nIf you have a car and enjoy driving, then driving from Tel-Aviv to Jerusalem is an option. However, parking in Jerusalem can be challenging and expensive.\\n\\nIf you prefer to use public trâ€¦</td><td>0</td><td>1</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ id     â”† model_a            â”† model_b             â”† prompt                                                                                          â”† â€¦ â”† response_b                                                                                      â”† winner_model_a â”† winner_model_b â”† winner_tie â”‚\n",
       "â”‚ ---    â”† ---                â”† ---                 â”† ---                                                                                             â”†   â”† ---                                                                                             â”† ---            â”† ---            â”† ---        â”‚\n",
       "â”‚ i64    â”† str                â”† str                 â”† str                                                                                             â”†   â”† str                                                                                             â”† i64            â”† i64            â”† i64        â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 30192  â”† gpt-4-1106-preview â”† gpt-4-0613          â”† [\"Is it morally right to try to have a certain percentage of females on managerial              â”† â€¦ â”† [\"As an AI, I don't have personal beliefs or opinions. However, I can tell you that the         â”† 1              â”† 0              â”† 0          â”‚\n",
       "â”‚        â”†                    â”†                     â”† positions?\",\"OK, does pineapple belong on a pizza? Relax and give me fun answer.\"]              â”†   â”† question of gender quotas in managerial positions is a complex one and positions can vary.      â”†                â”†                â”†            â”‚\n",
       "â”‚        â”†                    â”†                     â”†                                                                                                 â”†   â”† Advocates argue that such measures can help correct historical imbalances, promote diversity,   â”†                â”†                â”†            â”‚\n",
       "â”‚        â”†                    â”†                     â”†                                                                                                 â”†   â”† and may lead to better deciâ€¦                                                                    â”†                â”†                â”†            â”‚\n",
       "â”‚ 53567  â”† koala-13b          â”† gpt-4-0613          â”† [\"What is the difference between marriage license and marriage certificate?\",\"How can I get     â”† â€¦ â”† [\"A marriage license and a marriage certificate are two different legal documents that have     â”† 0              â”† 1              â”† 0          â”‚\n",
       "â”‚        â”†                    â”†                     â”† both of them as quick as possible in California \",\"What is the minimal time to get them? 1 day  â”†   â”† separate purposes.\\n\\n1. Marriage License: A marriage license is a legal document that gives a  â”†                â”†                â”†            â”‚\n",
       "â”‚        â”†                    â”†                     â”† or 10 days?\"]                                                                                   â”†   â”† couple permission to get married. It's usually obtained from local government or court          â”†                â”†                â”†            â”‚\n",
       "â”‚        â”†                    â”†                     â”†                                                                                                 â”†   â”† officials before the weddiâ€¦                                                                     â”†                â”†                â”†            â”‚\n",
       "â”‚ 65089  â”† gpt-3.5-turbo-0613 â”† mistral-medium      â”† [\"explain function calling. how would you call a function?\"]                                    â”† â€¦ â”† [\"Function calling is the process of invoking a function in a program. A function is a block of â”† 0              â”† 0              â”† 1          â”‚\n",
       "â”‚        â”†                    â”†                     â”†                                                                                                 â”†   â”† code that performs a specific task and can be reused multiple times in a program. Functions     â”†                â”†                â”†            â”‚\n",
       "â”‚        â”†                    â”†                     â”†                                                                                                 â”†   â”† help to organize and modularize code, making it more readable and maintainable.\\n\\nTo call a    â”†                â”†                â”†            â”‚\n",
       "â”‚        â”†                    â”†                     â”†                                                                                                 â”†   â”† function, you simplâ€¦                                                                            â”†                â”†                â”†            â”‚\n",
       "â”‚ 96401  â”† llama-2-13b-chat   â”† mistral-7b-instruct â”† [\"How can I create a test set for a very rare category? I want to build a classifier.\"]         â”† â€¦ â”† [\"When building a classifier for a very rare category, creating a test set can be challenging   â”† 1              â”† 0              â”† 0          â”‚\n",
       "â”‚        â”†                    â”†                     â”†                                                                                                 â”†   â”† due to the limited amount of data available. Here are some steps you can take to create a test  â”†                â”†                â”†            â”‚\n",
       "â”‚        â”†                    â”†                     â”†                                                                                                 â”†   â”† set for a rare category:\\n\\n1. Gather as much data as possible: The more data you have, the     â”†                â”†                â”†            â”‚\n",
       "â”‚        â”†                    â”†                     â”†                                                                                                 â”†   â”† easier it will be tâ€¦                                                                            â”†                â”†                â”†            â”‚\n",
       "â”‚ 198779 â”† koala-13b          â”† gpt-3.5-turbo-0314  â”† [\"What is the best way to travel from Tel-Aviv to Jerusalem? Car? Bus? Plane?\"]                 â”† â€¦ â”† [\"The best way to travel from Tel-Aviv to Jerusalem depends on your personal preferences and    â”† 0              â”† 1              â”† 0          â”‚\n",
       "â”‚        â”†                    â”†                     â”†                                                                                                 â”†   â”† circumstances. \\n\\nIf you have a car and enjoy driving, then driving from Tel-Aviv to Jerusalem â”†                â”†                â”†            â”‚\n",
       "â”‚        â”†                    â”†                     â”†                                                                                                 â”†   â”† is an option. However, parking in Jerusalem can be challenging and expensive.\\n\\nIf you prefer  â”†                â”†                â”†            â”‚\n",
       "â”‚        â”†                    â”†                     â”†                                                                                                 â”†   â”† to use public trâ€¦                                                                               â”†                â”†                â”†            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is it morally right to try to have a certain percentage of females on managerial positions?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"prompt\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Is it morally right to try to have a certain percentage of females on managerial positions?\",\"OK, does pineapple belong on a pizza? Relax and give me fun answer.\"]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"prompt\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- Split multi-turn columns to new rows\n",
    "  - ~~ensure len(resp) matches len(answer)~~\n",
    "  - ~~If any are not lists, handle~~\n",
    "  - ~~Remove `[]`~~\n",
    "  - ~~use id as user_id or convo_id~~\n",
    "  - ^ nevermind, thanks polars!\n",
    "  - preprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split each user/convo into a row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "if isinstance(df_train[\"prompt\"][0], str):\n",
    "    first_prompt_cell = df_train[\"prompt\"][0]\n",
    "    if \"[\" in first_prompt_cell and \"]\" in first_prompt_cell:\n",
    "        df_train = df_train.with_columns([pl.col(col).str.json_decode() for col in cols])\n",
    "        df_train = df_train.explode(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the baseline run, we're exploding the convos into each `user/assistant` turn. But originally, the user score was for a sometimes multi-turn conversation about often different subjects. Worth trying these merged back together at some point, since the score was for an aggregate, not each turn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any empty conversational turns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_clean(df: pl.DataFrame):\n",
    "    df = df.drop_nulls(subset=[\"prompt\", \"response_a\", \"response_b\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = null_clean(df_train)\n",
    "df_test = null_clean(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format for `BERT` by concatenating the prompt and both responses with `[SEP]` tokens in between\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is it morally right to try to have a certain percentage of females on managerial positions? [SEP] The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\\n\\nHere are some arguments in favor of and against such policies:\\n\\n**Arguments in favor:**\\n\\n1. **Correcting Historical Inequities:** Women have historically been underrepresented in leadership roles due to various cultural, institutional, and social barriers. Aiming for a specific percentage can be seen as a corrective measure to address past and ongoing discrimination.\\n\\n2. **Promoting Diversity:** Diverse leadership teams can enhance decision-making and represent a broader range of perspectives. This can lead to better outcomes for organizations and society as a whole.\\n\\n3. **Equality of Opportunity:** Setting targets for female representation in management can help ensure that women have equal opportunities to advance in their careers.\\n\\n4. **Role Modeling:** Increased visibility of female leaders can inspire and encourage other women and girls to pursue leadership roles.\\n\\n**Arguments against:**\\n\\n1. **Reverse Discrimination:** Setting a quota for female representation might lead to the perception or reality of reverse discrimination, where men might be overlooked for positions despite being qualified, simply to meet the gender target.\\n\\n2. **Meritocracy Concerns:** Critics argue that employment and promotion should be based solely on merit and qualifications, not gender. They fear that quotas could compromise the quality of leadership if less qualified individuals are promoted to meet gender targets.\\n\\n3. **Tokenism:** There is a risk that women appointed to meet quotas might be seen as \"tokens,\" undermining their legitimacy and potentially leading to resentment among colleagues.\\n\\n4. **Oversimplification of Diversity:** Focusing only on gender might overlook other important aspects of diversity, such as race, ethnicity, socioeconomic background, or sexual orientation.\\n\\nUltimately, the morality of aiming for a certain percentage of females in managerial positions depends on one\\'s ethical framework and the context in which such policies are implemented. Proponents of gender diversity targets often argue that these measures are necessary as a transitional mechanism to create a level playing field, while opponents may argue for a \"color-blind\" or \"gender-blind\" approach to hiring and promotions.\\n\\nOrganizations and societies must carefully consider the implications of these strategies and strive to implement policies that promote fairness and equality while minimizing negative consequences. It is also important to address the root causes of gender inequality, such as unconscious bias, work-life balance challenges, and lack of supportive policies, to create an environment where everyone has the opportunity to succeed based on their capabilities and contributions. [SEP] As an AI, I don\\'t have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decision-making. Critics may argue that positions should be based purely on merit, and that quotas could potentially lead to tokenism or unqualified candidates being promoted. Morality can be subjective and differs from person to person based on their personal beliefs, cultural background, and values. It\\'s a topic with valid arguments on both sides, and it\\'s up to each individual or organization to decide what they believe is right.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_train[\"prompt\"] + \" [SEP] \" + df_train[\"response_a\"] + \" [SEP] \" + df_train[\"response_b\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bert_fmt(df: pl.DataFrame):\n",
    "    df = df.with_columns(\n",
    "        (\n",
    "            pl.col(\"prompt\") + \" [SEP] \" + pl.col(\"response_a\") + \" [SEP] \" + pl.col(\"response_b\")\n",
    "        ).alias(\"text\")\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condense to a single target column and remap to 0,1,2 as possible dep vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.with_columns(\n",
    "    pl.when(pl.col(\"winner_model_a\") == 1)\n",
    "    .then(pl.lit(0))\n",
    "    .when(pl.col(\"winner_model_b\") == 1)\n",
    "    .then(pl.lit(1))\n",
    "    .when(pl.col(\"winner_tie\") == 1)\n",
    "    .then(pl.lit(2))\n",
    "    .otherwise(pl.lit(None))\n",
    "    .alias(\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = apply_bert_fmt(df_train)\n",
    "df_test = apply_bert_fmt(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>model_a</th><th>model_b</th><th>prompt</th><th>response_a</th><th>response_b</th><th>winner_model_a</th><th>winner_model_b</th><th>winner_tie</th><th>label</th><th>text</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i32</td><td>str</td></tr></thead><tbody><tr><td>30192</td><td>&quot;gpt-4-1106-preview&quot;</td><td>&quot;gpt-4-0613&quot;</td><td>&quot;Is it morally right to try to have a certain percentage of females on managerial positions?&quot;</td><td>&quot;The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\n",
       "\n",
       "Here are some arguments in favor of and against such policies:\n",
       "\n",
       "**Arguments inâ€¦</td><td>&quot;As an AI, I don&#x27;t have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decisiâ€¦</td><td>1</td><td>0</td><td>0</td><td>0</td><td>&quot;Is it morally right to try to have a certain percentage of females on managerial positions? [SEP] The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, aâ€¦</td></tr><tr><td>30192</td><td>&quot;gpt-4-1106-preview&quot;</td><td>&quot;gpt-4-0613&quot;</td><td>&quot;OK, does pineapple belong on a pizza? Relax and give me fun answer.&quot;</td><td>&quot;Ah, the age-old culinary conundrum that has divided nations and dinner tables: does pineapple belong on a pizza? The tropical twist of pineapple on pizza, known as Hawaiian pizza, is a hotly debated topic where taste buds battle and pizza purists protest.\n",
       "\n",
       "Let&#x27;s slice into the debate with a zest of â€¦</td><td>&quot;As an AI, I don&#x27;t eat, so I don&#x27;t have personal preferences. But, in the world of pizza, the pineapple topping is a hotly debated topic! Some people love the sweet and savory mix it brings to a pizza, a flavor combination made famous by the Hawaiian pizza. Others firmly believe that the sweetness ofâ€¦</td><td>1</td><td>0</td><td>0</td><td>0</td><td>&quot;OK, does pineapple belong on a pizza? Relax and give me fun answer. [SEP] Ah, the age-old culinary conundrum that has divided nations and dinner tables: does pineapple belong on a pizza? The tropical twist of pineapple on pizza, known as Hawaiian pizza, is a hotly debated topic where taste buds battâ€¦</td></tr><tr><td>53567</td><td>&quot;koala-13b&quot;</td><td>&quot;gpt-4-0613&quot;</td><td>&quot;What is the difference between marriage license and marriage certificate?&quot;</td><td>&quot;A marriage license is a legal document that allows a couple to get married. It is issued by a government agency, such as a county clerk&#x27;s office or a state government, and is valid for a certain period of time, usually one year. After the marriage has taken place, the couple must obtain a marriage câ€¦</td><td>&quot;A marriage license and a marriage certificate are two different legal documents that have separate purposes.\n",
       "\n",
       "1. Marriage License: A marriage license is a legal document that gives a couple permission to get married. It&#x27;s usually obtained from local government or court officials before the wedding câ€¦</td><td>0</td><td>1</td><td>0</td><td>1</td><td>&quot;What is the difference between marriage license and marriage certificate? [SEP] A marriage license is a legal document that allows a couple to get married. It is issued by a government agency, such as a county clerk&#x27;s office or a state government, and is valid for a certain period of time, usually oâ€¦</td></tr><tr><td>53567</td><td>&quot;koala-13b&quot;</td><td>&quot;gpt-4-0613&quot;</td><td>&quot;How can I get both of them as quick as possible in California &quot;</td><td>&quot;If you want to get both a marriage license and a marriage certificate in California as quickly as possible, there are a few steps you can follow:\n",
       "\n",
       "1.&nbsp;&nbsp;&nbsp;Obtain a marriage license: You can obtain a marriage license from the county clerk&#x27;s office in the county where you plan to marry. The process typicâ€¦</td><td>&quot;In California, here are the general steps to obtain a marriage license and marriage certificate as quickly as possible:\n",
       "\n",
       "1. Marriage License:\n",
       "\n",
       "&nbsp;&nbsp;&nbsp;- Both parties must appear together at one of the County Clerkâ€™s offices.\n",
       "&nbsp;&nbsp;&nbsp;- Complete a marriage license application before arriving (available online iâ€¦</td><td>0</td><td>1</td><td>0</td><td>1</td><td>&quot;How can I get both of them as quick as possible in California&nbsp;&nbsp;[SEP] If you want to get both a marriage license and a marriage certificate in California as quickly as possible, there are a few steps you can follow:\n",
       "\n",
       "1.&nbsp;&nbsp;&nbsp;Obtain a marriage license: You can obtain a marriage license from the county clâ€¦</td></tr><tr><td>53567</td><td>&quot;koala-13b&quot;</td><td>&quot;gpt-4-0613&quot;</td><td>&quot;What is the minimal time to get them? 1 day or 10 days?&quot;</td><td>&quot;The minimal time to get a marriage license and a marriage certificate in California can vary depending on the specific requirements and process in your county. Generally, you can obtain a marriage license and a marriage certificate in California within a few days of applying, but it is best to checkâ€¦</td><td>&quot;The marriage license in California is issued immediately when you apply in person and meet all the requirements, so theoretically, you could get married on the same day if you have an officiant ready to perform the ceremony.\n",
       "\n",
       "However, the marriage certificate, which is the official record of your maâ€¦</td><td>0</td><td>1</td><td>0</td><td>1</td><td>&quot;What is the minimal time to get them? 1 day or 10 days? [SEP] The minimal time to get a marriage license and a marriage certificate in California can vary depending on the specific requirements and process in your county. Generally, you can obtain a marriage license and a marriage certificate in Calâ€¦</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 11)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ id    â”† model_a            â”† model_b    â”† prompt                                                                                      â”† â€¦ â”† winner_model_b â”† winner_tie â”† label â”† text                                                                                                                   â”‚\n",
       "â”‚ ---   â”† ---                â”† ---        â”† ---                                                                                         â”†   â”† ---            â”† ---        â”† ---   â”† ---                                                                                                                    â”‚\n",
       "â”‚ i64   â”† str                â”† str        â”† str                                                                                         â”†   â”† i64            â”† i64        â”† i32   â”† str                                                                                                                    â”‚\n",
       "â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 30192 â”† gpt-4-1106-preview â”† gpt-4-0613 â”† Is it morally right to try to have a certain percentage of females on managerial positions? â”† â€¦ â”† 0              â”† 0          â”† 0     â”† Is it morally right to try to have a certain percentage of females on managerial positions? [SEP] The question of      â”‚\n",
       "â”‚       â”†                    â”†            â”†                                                                                             â”†   â”†                â”†            â”†       â”† whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical    â”‚\n",
       "â”‚       â”†                    â”†            â”†                                                                                             â”†   â”†                â”†            â”†       â”† issue that involves considerations of fairness, equality, diversity, aâ€¦                                                â”‚\n",
       "â”‚ 30192 â”† gpt-4-1106-preview â”† gpt-4-0613 â”† OK, does pineapple belong on a pizza? Relax and give me fun answer.                         â”† â€¦ â”† 0              â”† 0          â”† 0     â”† OK, does pineapple belong on a pizza? Relax and give me fun answer. [SEP] Ah, the age-old culinary conundrum that has  â”‚\n",
       "â”‚       â”†                    â”†            â”†                                                                                             â”†   â”†                â”†            â”†       â”† divided nations and dinner tables: does pineapple belong on a pizza? The tropical twist of pineapple on pizza, known   â”‚\n",
       "â”‚       â”†                    â”†            â”†                                                                                             â”†   â”†                â”†            â”†       â”† as Hawaiian pizza, is a hotly debated topic where taste buds battâ€¦                                                     â”‚\n",
       "â”‚ 53567 â”† koala-13b          â”† gpt-4-0613 â”† What is the difference between marriage license and marriage certificate?                   â”† â€¦ â”† 1              â”† 0          â”† 1     â”† What is the difference between marriage license and marriage certificate? [SEP] A marriage license is a legal document â”‚\n",
       "â”‚       â”†                    â”†            â”†                                                                                             â”†   â”†                â”†            â”†       â”† that allows a couple to get married. It is issued by a government agency, such as a county clerk's office or a state   â”‚\n",
       "â”‚       â”†                    â”†            â”†                                                                                             â”†   â”†                â”†            â”†       â”† government, and is valid for a certain period of time, usually oâ€¦                                                      â”‚\n",
       "â”‚ 53567 â”† koala-13b          â”† gpt-4-0613 â”† How can I get both of them as quick as possible in California                               â”† â€¦ â”† 1              â”† 0          â”† 1     â”† How can I get both of them as quick as possible in California  [SEP] If you want to get both a marriage license and a  â”‚\n",
       "â”‚       â”†                    â”†            â”†                                                                                             â”†   â”†                â”†            â”†       â”† marriage certificate in California as quickly as possible, there are a few steps you can follow:                       â”‚\n",
       "â”‚       â”†                    â”†            â”†                                                                                             â”†   â”†                â”†            â”†       â”†                                                                                                                        â”‚\n",
       "â”‚       â”†                    â”†            â”†                                                                                             â”†   â”†                â”†            â”†       â”† 1.   Obtain a marriage license: You can obtain a marriage license from the county clâ€¦                                  â”‚\n",
       "â”‚ 53567 â”† koala-13b          â”† gpt-4-0613 â”† What is the minimal time to get them? 1 day or 10 days?                                     â”† â€¦ â”† 1              â”† 0          â”† 1     â”† What is the minimal time to get them? 1 day or 10 days? [SEP] The minimal time to get a marriage license and a         â”‚\n",
       "â”‚       â”†                    â”†            â”†                                                                                             â”†   â”†                â”†            â”†       â”† marriage certificate in California can vary depending on the specific requirements and process in your county.         â”‚\n",
       "â”‚       â”†                    â”†            â”†                                                                                             â”†   â”†                â”†            â”†       â”† Generally, you can obtain a marriage license and a marriage certificate in Calâ€¦                                        â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the training data into training and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_train[\"text\"], df_train[\"label\"], test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64205,), (7134,), (64205,), (7134,))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.shape, val_texts.shape, train_labels.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a code competition, which means that internet access is cut off when running the notebook. Some people have kindly created kaggle `datasets` of some `BERT` models. I've added the relevant dataset as a dependecy, so in theory this will use a \"local\" version of the model in kaggle.\n",
    "\n",
    "This doesn't always work perfectly. Sometimes it needs the nudge of saving it as a new draft on kaggle to give the `dataset` time to load.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if iskaggle:\n",
    "    model_path = \"../input/huggingface-bert/bert-base-uncased\"\n",
    "    print(\"Loading BERT from Kaggle model input...\")\n",
    "\n",
    "else:\n",
    "    model_path = \"bert-base-uncased\"\n",
    "    print(\"Loading BERT from Hugging Face...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "model.to(device)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2632, 14855, 23940, 2527, 2003, 2013, 2073, 1029, 102, 2632, 14855, 23940, 2527, 2003, 1037, 4069, 3795, 2865, 2897, 2761, 2013, 26528, 1010, 1996, 3007, 2103, 1997, 12577, 1012, 2009, 2001, 3390, 2006, 2281, 1015, 1010, 2727, 1010, 2011, 1996, 23434, 1997, 12577, 2012, 1996, 2051, 1010, 12840, 10654, 4215, 8026, 27925, 2632, 2084, 2072, 1012, 1996, 2171, 1000, 2632, 14855, 23940, 2527, 1000, 16315, 2000, 1000, 1996, 6000, 1000, 1999, 2394, 1010, 7727, 2000, 1996, 13771, 6000, 1012, 2632, 14855, 23940, 2527, 2003, 2124, 2005, 2049, 2981, 1998, 2411, 4187, 6325, 1997, 2739, 1998, 2783, 3821, 1010, 2119, 2306, 1996, 2690, 2264, 1998, 2105, 1996, 2088, 1012, 102, 2632, 14855, 23940, 2527, 2003, 1037, 2865, 2897, 2241, 1999, 26528, 1010, 12577, 1012, 2009, 2001, 3390, 1999, 2727, 1998, 2038, 4961, 2000, 2022, 1037, 2350, 3795, 2739, 3029, 1010, 2007, 3674, 2694, 6833, 1998, 3617, 7248, 4346, 2739, 6325, 1999, 5640, 1010, 2394, 1010, 1998, 2060, 4155, 1012, 2295, 2009, 2003, 3079, 2011, 1996, 2231, 1997, 12577, 1010, 2632, 14855, 23940, 2527, 5927, 2993, 2004, 2019, 2981, 2739, 3120, 4346, 7863, 8083, 2013, 2019, 5424, 1998, 12368, 3795, 2148, 7339, 1012, 2049, 4075, 1998, 2434, 5640, 2653, 3149, 2024, 2284, 1999, 26528, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_enc = tokenizer(train_texts[0], truncation=True, padding=True)\n",
    "ex_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer, labels=None, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    # encoding on the fly here due to issues with memory on kaggle\n",
    "    # when pre-tokenizing\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LLMDataset(list(train_texts), tokenizer, list(train_labels))\n",
    "val_dataset = LLMDataset(list(val_texts), tokenizer, list(val_labels))\n",
    "test_dataset = LLMDataset(list(df_test[\"text\"]), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeterbull\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/peterbull/peter-projects/kaggle/llm-classification-finetuning/nbs/wandb/run-20250608_061401-jlgou3tt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peterbull/kaggle-llm-classification/runs/jlgou3tt' target=\"_blank\">bert-classification-20250608-0614</a></strong> to <a href='https://wandb.ai/peterbull/kaggle-llm-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peterbull/kaggle-llm-classification' target=\"_blank\">https://wandb.ai/peterbull/kaggle-llm-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peterbull/kaggle-llm-classification/runs/jlgou3tt' target=\"_blank\">https://wandb.ai/peterbull/kaggle-llm-classification/runs/jlgou3tt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:06, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.096400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.114400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=1.1179282426834107, metrics={'train_runtime': 8.4038, 'train_samples_per_second': 9.52, 'train_steps_per_second': 1.19, 'total_flos': 21049073418240.0, 'train_loss': 1.1179282426834107, 'epoch': 0.00124595066035385})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model_path = f\"{MODEL_DIR}/final\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "run_name = f\"bert-classification-{timestamp}\"\n",
    "if not iskaggle:\n",
    "    # Quick run to test pipeline\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/results\",\n",
    "        run_name=run_name,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        max_steps=10,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        logging_steps=2,\n",
    "        logging_first_step=True,\n",
    "        report_to=\"wandb\" if os.environ.get(\"WANDB_MODE\") != \"disabled\" else [],\n",
    "        dataloader_num_workers=0,  # Important for Kaggle compatibility\n",
    "    )\n",
    "\n",
    "else:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/results\",\n",
    "        run_name=run_name,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        logging_steps=20,\n",
    "        logging_first_step=True,\n",
    "        report_to=\"wandb\" if os.environ.get(\"WANDB_MODE\") != \"disabled\" else [],\n",
    "        dataloader_num_workers=0,  # Important for Kaggle compatibility\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# trainer.save_model(final_model_path)\n",
    "# tokenizer.save_pretrained(final_model_path)\n",
    "# if os.environ.get(\"WANDB_MODE\") != \"disabled\":\n",
    "#     wandb.log({\"final_eval\": eval_results})\n",
    "#     wandb.save(f\"{final_model_path}/*\")\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1998, -0.2360, -0.3675]], device='mps:0',\n",
       "       grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model(model_path):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "model, tokenizer = load_model(final_model_path)\n",
    "model.to(device)\n",
    "\n",
    "text = \"This is a test sentence\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = model(**inputs.to(device))\n",
    "predictions = outputs.logits\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3559, 0.3432, 0.3009]], device='mps:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = F.softmax(predictions, dim=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3142799 , 0.3925321 , 0.29318798],\n",
       "       [0.34361842, 0.35338566, 0.30299595],\n",
       "       [0.30349636, 0.3418722 , 0.35463145]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_probabilities = []\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs = {k: v.to(model.device) for k, v in batch.items() if k != \"labels\"}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "final_probs = np.vstack(all_probabilities)\n",
    "final_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3142799 , 0.34361842, 0.30349636], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_probs[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>winner_model_a</th><th>winner_model_b</th><th>winner_tie</th></tr><tr><td>i64</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>136060</td><td>0.31428</td><td>0.392532</td><td>0.293188</td></tr><tr><td>211333</td><td>0.343618</td><td>0.353386</td><td>0.302996</td></tr><tr><td>1233961</td><td>0.303496</td><td>0.341872</td><td>0.354631</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 4)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ id      â”† winner_model_a â”† winner_model_b â”† winner_tie â”‚\n",
       "â”‚ ---     â”† ---            â”† ---            â”† ---        â”‚\n",
       "â”‚ i64     â”† f32            â”† f32            â”† f32        â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 136060  â”† 0.31428        â”† 0.392532       â”† 0.293188   â”‚\n",
       "â”‚ 211333  â”† 0.343618       â”† 0.353386       â”† 0.302996   â”‚\n",
       "â”‚ 1233961 â”† 0.303496       â”† 0.341872       â”† 0.354631   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = df_test\n",
    "submission_df = submission_df.with_columns(\n",
    "    pl.lit(final_probs[:, 0]).alias(\"winner_model_a\"),\n",
    "    pl.lit(final_probs[:, 1]).alias(\"winner_model_b\"),\n",
    "    pl.lit(final_probs[:, 2]).alias(\"winner_tie\"),\n",
    ")\n",
    "submission_df = submission_df[[\"id\", \"winner_model_a\", \"winner_model_b\", \"winner_tie\"]]\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_kaggle = submission_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.314280</td>\n",
       "      <td>0.392532</td>\n",
       "      <td>0.293188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.343618</td>\n",
       "      <td>0.353386</td>\n",
       "      <td>0.302996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.303496</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.354631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "0   136060        0.314280        0.392532    0.293188\n",
       "1   211333        0.343618        0.353386    0.302996\n",
       "2  1233961        0.303496        0.341872    0.354631"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_kaggle.to_csv(\"submission.csv\", index=False)\n",
    "df_for_kaggle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Notebook to Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing to Kaggle...\n",
      "âœ… Notebook pushed successfully!\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/peterbull/.kaggle/kaggle.json'\n",
      "Kernel version 29 successfully pushed.  Please check progress at https://www.kaggle.com/code/peterbull/llm-classification-bert-finetuning\n",
      "\n",
      "ğŸ”— View at: https://www.kaggle.com/code/peterbull/llm-classification-bert-finetuning\n"
     ]
    }
   ],
   "source": [
    "def push_notebook_cli():\n",
    "    username = \"peterbull\"\n",
    "    comp = \"llm-classification-finetuning\"\n",
    "\n",
    "    metadata = {\n",
    "        \"id\": f\"{username}/llm-classification-bert-finetuning\",\n",
    "        \"title\": \"LLM Classification BERT Finetuning\",\n",
    "        \"code_file\": \"20250603_base_bert.ipynb\",\n",
    "        \"language\": \"python\",\n",
    "        \"kernel_type\": \"notebook\",\n",
    "        \"is_private\": False,\n",
    "        \"enable_gpu\": True,\n",
    "        \"enable_internet\": False,  # required for kaggle code competition\n",
    "        \"dataset_sources\": [],\n",
    "        \"competition_sources\": [f\"competitions/{comp}\"],\n",
    "        \"kernel_sources\": [],\n",
    "    }\n",
    "\n",
    "    with open(\"kernel-metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    if not os.path.exists(\"20250603_base_bert.ipynb\"):\n",
    "        print(\" Notebook file '20250603_base_bert.ipynb' not found!\")\n",
    "        print(\" Files in current directory:\")\n",
    "        for f in os.listdir(\".\"):\n",
    "            if f.endswith(\".ipynb\"):\n",
    "                print(f\"{f}\")\n",
    "        return\n",
    "\n",
    "    print(\"Pushing to Kaggle...\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"kaggle\", \"kernels\", \"push\", \"-p\", \".\"], capture_output=True, text=True, timeout=300\n",
    "        )\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Notebook pushed successfully!\")\n",
    "            print(result.stdout)\n",
    "            print(\n",
    "                f\"ğŸ”— View at: https://www.kaggle.com/code/{username}/llm-classification-bert-finetuning\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"Error pushing notebook:\")\n",
    "            print(result.stderr)\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"Upload timed out after 5 minutes\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Kaggle CLI not found. Install with: pip install kaggle\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "if not iskaggle:\n",
    "    push_notebook_cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
